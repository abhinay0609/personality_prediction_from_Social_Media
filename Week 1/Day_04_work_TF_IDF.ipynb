{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/bhDyCqNfsqO68EI3Nqu2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cB-yRDJz86-3","executionInfo":{"status":"ok","timestamp":1749745608745,"user_tz":-330,"elapsed":6052,"user":{"displayName":"Abhinay Singh","userId":"04074215444349371949"}},"outputId":"e0daa2bf-4e55-4e32-de94-a562a6df2d33"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Cleaned Tokens: ['social', 'media', 'affect', 'positive', 'nagative', 'ways']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["from IPython import get_ipython\n","from IPython.display import display\n","# %%\n","# Ensure NLTK is installed\n","%pip install nltk\n","# %%\n","import nltk\n","import shutil\n","import string\n","from nltk.corpus import stopwords\n","\n","# Remove the nltk_data directory to ensure a fresh download\n","# This step is okay if you want to guarantee a fresh download, but be mindful of its side effects\n","shutil.rmtree('/root/nltk_data', ignore_errors=True)\n","\n","# %%\n","# Download the required resources *after* removing the directory\n","# Force the download to ensure it happens\n","nltk.download('punkt', force=True)\n","nltk.download('stopwords', force=True)\n","\n","# %%\n","# text = \"I LOVE AI and Machine Learning!! It's so exciting.\"\n","text = \"Social Media can affect in both positive and nagative ways.\"\n","\n","# Lowercase\n","text = text.lower()\n","\n","# Remove punctuation\n","text = ''.join([char for char in text if char not in string.punctuation])\n","\n","# Tokenize using split (not word_tokenize)\n","tokens = text.split()\n","\n","# Remove stopwords\n","stop_words = set(stopwords.words('english'))\n","clean_tokens = [word for word in tokens if word not in stop_words]\n","\n","print(\"Cleaned Tokens:\", clean_tokens)"]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Sample sentences (you can imagine them as posts or tweets)\n","texts = [\n","    \"I love machine learning and AI\",\n","    \"AI is the future of technology\",\n","    \"I enjoy learning new things about AI\"\n","]\n","\n","# Create the TF-IDF Vectorizer\n","# vectorizer = TfidfVectorizer()\n","vectorizer = TfidfVectorizer(stop_words='english')\n","\n","\n","# Fit and transform the text data\n","tfidf_matrix = vectorizer.fit_transform(texts)\n","\n","# Convert matrix to array\n","tfidf_array = tfidf_matrix.toarray()\n","\n","# Get the feature (word) names\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Display results\n","import pandas as pd\n","df = pd.DataFrame(tfidf_array, columns=feature_names)\n","print(df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RvABsXr9t8j","executionInfo":{"status":"ok","timestamp":1749748968211,"user_tz":-330,"elapsed":14,"user":{"displayName":"Abhinay Singh","userId":"04074215444349371949"}},"outputId":"d4b3012b-c572-4bf4-c78a-9bbc7dadd1bb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["         ai     enjoy    future  learning      love   machine       new  \\\n","0  0.345205  0.000000  0.000000  0.444514  0.584483  0.584483  0.000000   \n","1  0.385372  0.000000  0.652491  0.000000  0.000000  0.000000  0.000000   \n","2  0.298032  0.504611  0.000000  0.383770  0.000000  0.000000  0.504611   \n","\n","   technology    things  \n","0    0.000000  0.000000  \n","1    0.652491  0.000000  \n","2    0.000000  0.504611  \n"]}]}]}